{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_json = json.load(open(\"cran_docs.json\", 'r'))[:]\n",
    "doc_ids, docs = [item[\"id\"] for item in docs_json],[item[\"body\"] for item in docs_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_json = json.load(open(\"cran_queries.json\", 'r'))[:]\n",
    "query_ids, queries = [item[\"query number\"] for item in queries_json],[item[\"query\"] for item in queries_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = json.load(open(\"cran_qrels.json\", 'r'))[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = RegexpTokenizer(r'\\b\\w{1,}\\b')\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`', '(', ')', '+', '-', '--', '*', '/', '']\n",
    "    def __init__(self):\n",
    "        self.wnl = PorterStemmer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.stem(re.sub(r'[0-9]', \"\", t)) for t in tokenize.tokenize(articles) if re.sub(r'[0-9]', \"\", t) not in self.ignore_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=LemmaTokenizer()\n",
    "token_stop = tokenizer(' '.join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 4287\n",
    "d = 1390\n",
    "p = 10\n",
    "k = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "sample_tfidf = TfidfVectorizer(lowercase = True, stop_words = token_stop, tokenizer = tokenizer) #, min_df = 0.1)\n",
    "sample_sparse = sample_tfidf.fit_transform(docs[:d])\n",
    "tfidf = sample_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390, 4287)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.transpose(tfidf)\n",
    "#A = Ahat[:,:d]\n",
    "D = sample_tfidf.transform(docs[d:])\n",
    "D = np.transpose(D.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4287, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(A, full_matrices = False)\n",
    "Uk = u[:,:k]\n",
    "Sk = np.diag(s[:k])\n",
    "Vk = np.transpose(vh[:k,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dcap = np.matmul(np.ones((t,t)) - np.matmul(Uk, np.transpose(Uk)), D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dcap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qd, Rd = np.linalg.qr(Dcap, mode='reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.append(Sk, np.matmul(np.transpose(Uk), D), axis = 1)\n",
    "#b = np.append(np.zeros((p, k)), Rd, axis = 1)\n",
    "Acap = np.append(Sk, np.matmul(np.transpose(Uk), D), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 310)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Acap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ucap, Scap, Vcapt = np.linalg.svd(Acap, full_matrices = False)\n",
    "Vcap = np.transpose(Vcapt)\n",
    "#Ukcap = u[:,:k]\n",
    "#Upcap = u[:,k:]\n",
    "#Skcap = np.diag(s[:k])\n",
    "#Spcap = np.diag(s[k:])\n",
    "#Vkcap = np.transpose(vh)[:,:k]\n",
    "#Vpcap = np.transpose(vh)[:,k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.matmul(Uk, Ucap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.diag(Scap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(Vk, np.zeros((d, p)), axis = 1)\n",
    "b = np.append(np.zeros((p, k)), np.ones((p,p)), axis = 1)\n",
    "V = np.matmul(np.append(a, b, axis = 0), Vcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dt = np.transpose(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sparse_q = sample_tfidf.transform(queries)\n",
    "tfidf_q = sample_sparse_q.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSI(n_components, u, s, vh, tfidf_q):\n",
    "    T = u[:,:n_components]\n",
    "    S = np.diag(s[:n_components])\n",
    "    Dt = vh[:n_components,:]\n",
    "    doc_vectors = np.dot(np.transpose(Dt), S)\n",
    "    query_vectors = np.dot(tfidf_q, T)\n",
    "    return doc_vectors, query_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(doc_vectors, query_vectors, docIDs):\n",
    "    doc_IDs_ordered = list()\n",
    "    for q_vector in query_vectors:\n",
    "        retrieved_docs = dict()\n",
    "        key = 0\n",
    "        for doc_vector in doc_vectors:\n",
    "            cosine = 1 - distance.cosine(doc_vector, q_vector)\n",
    "            retrieved_docs[docIDs[key]] = cosine\n",
    "            key += 1\n",
    "        doc_IDs_ordered.append(sorted(retrieved_docs,reverse=True,key = lambda x: retrieved_docs[x]))\n",
    "    return doc_IDs_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation():\n",
    "\n",
    "\tdef queryPrecision(self, query_doc_IDs_ordered, query_id, true_doc_IDs, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of precision of the Information Retrieval System\n",
    "\t\tat a given value of k for a single query\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of integers denoting the IDs of documents in\n",
    "\t\t\ttheir predicted order of relevance to a query\n",
    "\t\targ2 : int\n",
    "\t\t\tThe ID of the query in question\n",
    "\t\targ3 : list\n",
    "\t\t\tThe list of IDs of documents relevant to the query (ground truth)\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe precision value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tprecision = -1\n",
    "\t\tretrieved_docs = query_doc_IDs_ordered[:k]\n",
    "\t\tn_relevant_docs = 0\n",
    "\t\t\n",
    "\t\tfor doc in retrieved_docs:\n",
    "\t\t\tif doc in true_doc_IDs:\n",
    "\t\t\t\tn_relevant_docs+=1\n",
    "\t\t\n",
    "\t\tprecision = n_relevant_docs/k\n",
    "\n",
    "\t\treturn precision\n",
    "\n",
    "\n",
    "\tdef meanPrecision(self, doc_IDs_ordered, query_ids, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of precision of the Information Retrieval System\n",
    "\t\tat a given value of k, averaged over all the queries\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of lists of integers where the ith sub-list is a list of IDs\n",
    "\t\t\tof documents in their predicted order of relevance to the ith query\n",
    "\t\targ2 : list\n",
    "\t\t\tA list of IDs of the queries for which the documents are ordered\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe mean precision value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmeanPrecision = -1\n",
    "\t\ttotal_Precision = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in query_ids:\n",
    "\t\t\ttrue_doc_IDs = []\n",
    "\t\t\tflag = 0\n",
    "\t\t\tfor dic in qrels:\n",
    "\t\t\t\tif(int(dic[\"query_num\"]) == i):\n",
    "\t\t\t\t\tflag = 1\n",
    "\t\t\t\t\ttrue_doc_IDs.append(int(dic[\"id\"]))\n",
    "\t\t\t\telif(flag == 1):\n",
    "\t\t\t\t\tbreak     \n",
    "\t\t\ttotal_Precision = total_Precision + self.queryPrecision(doc_IDs_ordered[count] , i , true_doc_IDs, k)\n",
    "\t\t\tcount = count+1\n",
    "\t\tif count == 0:\n",
    "\t  \t\tcount = 1\n",
    "\t\tmeanPrecision = total_Precision/count\n",
    "\n",
    "\t\treturn meanPrecision\n",
    "\n",
    "\t\n",
    "\tdef queryRecall(self, query_doc_IDs_ordered, query_id, true_doc_IDs, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of recall of the Information Retrieval System\n",
    "\t\tat a given value of k for a single query\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of integers denoting the IDs of documents in\n",
    "\t\t\ttheir predicted order of relevance to a query\n",
    "\t\targ2 : int\n",
    "\t\t\tThe ID of the query in question\n",
    "\t\targ3 : list\n",
    "\t\t\tThe list of IDs of documents relevant to the query (ground truth)\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe recall value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\trecall = -1\n",
    "\t\tretrieved_docs = query_doc_IDs_ordered[:k]\n",
    "\t\tn_relevant_docs = 0\n",
    "\t\tfor doc in retrieved_docs:\n",
    "\t\t\tif doc in  true_doc_IDs:\n",
    "\t\t\t\tn_relevant_docs += 1\n",
    "\t\trecall = n_relevant_docs/len(true_doc_IDs)\n",
    "\n",
    "\t\treturn recall\n",
    "\n",
    "\n",
    "\tdef meanRecall(self, doc_IDs_ordered, query_ids, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of recall of the Information Retrieval System\n",
    "\t\tat a given value of k, averaged over all the queries\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of lists of integers where the ith sub-list is a list of IDs\n",
    "\t\t\tof documents in their predicted order of relevance to the ith query\n",
    "\t\targ2 : list\n",
    "\t\t\tA list of IDs of the queries for which the documents are ordered\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe mean recall value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmeanRecall = -1\n",
    "\t\ttotal_Recall = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in query_ids:\n",
    "\t\t\ttrue_doc_IDs = []\n",
    "\t\t\tflag = 0\n",
    "\t\t\tfor dic in qrels:\n",
    "\t\t\t\tif(int(dic[\"query_num\"]) == i):\n",
    "\t\t\t\t\tflag = 1\n",
    "\t\t\t\t\ttrue_doc_IDs.append(int(dic[\"id\"]))\n",
    "\t\t\t\telif(flag == 1):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\ttotal_Recall = total_Recall + self.queryRecall(doc_IDs_ordered[count] , i , true_doc_IDs, k)\n",
    "\t\t\tcount = count+1\n",
    "\t\tif count == 0:\n",
    "\t  \t\tcount = 1\n",
    "\t\tmeanRecall = total_Recall/count\n",
    "\n",
    "\t\treturn meanRecall\n",
    "\n",
    "\n",
    "\tdef queryFscore(self, query_doc_IDs_ordered, query_id, true_doc_IDs, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of fscore of the Information Retrieval System\n",
    "\t\tat a given value of k for a single query\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of integers denoting the IDs of documents in\n",
    "\t\t\ttheir predicted order of relevance to a query\n",
    "\t\targ2 : int\n",
    "\t\t\tThe ID of the query in question\n",
    "\t\targ3 : list\n",
    "\t\t\tThe list of IDs of documents relevant to the query (ground truth)\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe fscore value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tfscore = -1\n",
    "\t\tprecision = self.queryPrecision(query_doc_IDs_ordered , query_id , true_doc_IDs, k)\n",
    "\t\trecall = self.queryRecall(query_doc_IDs_ordered , query_id , true_doc_IDs, k)\n",
    "\t\tfscore = (2*precision*recall)/(precision + recall + 0.00001)\n",
    "\n",
    "\t\treturn fscore\n",
    "\n",
    "\n",
    "\tdef meanFscore(self, doc_IDs_ordered, query_ids, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of fscore of the Information Retrieval System\n",
    "\t\tat a given value of k, averaged over all the queries\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of lists of integers where the ith sub-list is a list of IDs\n",
    "\t\t\tof documents in their predicted order of relevance to the ith query\n",
    "\t\targ2 : list\n",
    "\t\t\tA list of IDs of the queries for which the documents are ordered\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe mean fscore value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmeanFscore = -1\n",
    "\t\ttotal_Fscore = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in query_ids:\n",
    "\t\t\ttrue_doc_IDs = []\n",
    "\t\t\tflag = 0\n",
    "\t\t\tfor dic in qrels:\n",
    "\t\t\t\tif(int(dic[\"query_num\"]) == i):\n",
    "\t\t\t\t\tflag = 1\n",
    "\t\t\t\t\ttrue_doc_IDs.append(int(dic[\"id\"]))\n",
    "\t\t\t\telif(flag == 1):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\ttotal_Fscore = total_Fscore + self.queryFscore(doc_IDs_ordered[count] , i , true_doc_IDs, k)\n",
    "\t\t\tcount = count+1\n",
    "\t\tif count == 0:\n",
    "\t  \t\tcount = 1\n",
    "\t\tmeanFscore = total_Fscore/count\n",
    "\n",
    "\t\treturn meanFscore\n",
    "\t\n",
    "\n",
    "\tdef queryNDCG(self, query_doc_IDs_ordered, query_id, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of nDCG of the Information Retrieval System\n",
    "\t\tat given value of k for a single query\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of integers denoting the IDs of documents in\n",
    "\t\t\ttheir predicted order of relevance to a query\n",
    "\t\targ2 : int\n",
    "\t\t\tThe ID of the query in question\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe nDCG value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tnDCG = -1\n",
    "\t\tDCG = 0\n",
    "\t\tiDCG = 0\n",
    "\t\tretrieved_docs = query_doc_IDs_ordered[:k]\n",
    "\t\ttrue_doc_IDs = dict()\n",
    "\t\tflag = 0\n",
    "\t\t\n",
    "\t\tfor dic in qrels:\n",
    "\t\t\tif(int(dic[\"query_num\"]) == query_id):\n",
    "\t\t\t\tflag = 1\n",
    "\t\t\t\ttrue_doc_IDs[int(dic[\"id\"])] = 5 - dic['position']\n",
    "\t\t\telif(flag == 1):\n",
    "\t\t\t\tbreak\n",
    "\t\tideal_order = sorted(query_doc_IDs_ordered, key = lambda x: true_doc_IDs[x] if x in true_doc_IDs else 0, reverse = True)[:k]\n",
    "\t\tfor i in range(1,k+1):\n",
    "\t\t\tif retrieved_docs[i-1] in true_doc_IDs:\n",
    "\t\t\t\tDCG += true_doc_IDs[retrieved_docs[i-1]]/math.log2(i+1)\n",
    "\t\t\tif ideal_order[i-1] in true_doc_IDs:\n",
    "\t\t\t\tiDCG += true_doc_IDs[ideal_order[i-1]]/math.log2(i+1)\n",
    "\n",
    "\t\tif iDCG == 0:\n",
    "\t\t\tiDCG = 1\t\n",
    "\t\tnDCG = DCG/iDCG\n",
    "\n",
    "\t\treturn nDCG\n",
    "\n",
    "\n",
    "\tdef meanNDCG(self, doc_IDs_ordered, query_ids, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of nDCG of the Information Retrieval System\n",
    "\t\tat a given value of k, averaged over all the queries\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of lists of integers where the ith sub-list is a list of IDs\n",
    "\t\t\tof documents in their predicted order of relevance to the ith query\n",
    "\t\targ2 : list\n",
    "\t\t\tA list of IDs of the queries for which the documents are ordered\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe mean nDCG value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmeanNDCG = -1\n",
    "\t\ttotal_NDCG = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in query_ids:\n",
    "\t\t\ttotal_NDCG = total_NDCG + self.queryNDCG(doc_IDs_ordered[count] , i , qrels, k)\n",
    "\t\t\tcount = count+1\n",
    "\t\tif count == 0:\n",
    "\t  \t\tmeanNDCG = 0\n",
    "\t\telse:\n",
    "\t\t\tmeanNDCG = total_NDCG/count\n",
    "\n",
    "\t\treturn meanNDCG\n",
    "\n",
    "\n",
    "\tdef queryAveragePrecision(self, query_doc_IDs_ordered, query_id, true_doc_IDs, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of average precision of the Information Retrieval System\n",
    "\t\tat a given value of k for a single query (the average of precision@i\n",
    "\t\tvalues for i such that the ith document is truly relevant)\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of integers denoting the IDs of documents in\n",
    "\t\t\ttheir predicted order of relevance to a query\n",
    "\t\targ2 : int\n",
    "\t\t\tThe ID of the query in question\n",
    "\t\targ3 : list\n",
    "\t\t\tThe list of documents relevant to the query (ground truth)\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe average precision value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tavgPrecision = -1\n",
    "\t\tretrieved_docs = query_doc_IDs_ordered[:k]\n",
    "\t\tTotal_Precision = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in range(len(retrieved_docs)):\n",
    "\t\t\tdoc = retrieved_docs[i]\n",
    "\t\t\tif doc in true_doc_IDs:\n",
    "\t\t\t\tcount += 1\n",
    "\t\t\t\tTotal_Precision += self.queryPrecision(query_doc_IDs_ordered, query_id, true_doc_IDs, i+1) \n",
    "\t\tif count == 0:\n",
    "\t\t\tavgPrecision = 0\n",
    "\t\telse:      \n",
    "\t\t\tavgPrecision = Total_Precision/count\n",
    "\n",
    "\t\treturn avgPrecision\n",
    "\n",
    "\n",
    "\tdef meanAveragePrecision(self, doc_IDs_ordered, query_ids, qrels, k):\n",
    "\t\t\"\"\"\n",
    "\t\tComputation of MAP of the Information Retrieval System\n",
    "\t\tat given value of k, averaged over all the queries\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\targ1 : list\n",
    "\t\t\tA list of lists of integers where the ith sub-list is a list of IDs\n",
    "\t\t\tof documents in their predicted order of relevance to the ith query\n",
    "\t\targ2 : list\n",
    "\t\t\tA list of IDs of the queries\n",
    "\t\targ3 : list\n",
    "\t\t\tA list of dictionaries containing document-relevance\n",
    "\t\t\tjudgements - Refer cran_qrels.json for the structure of each\n",
    "\t\t\tdictionary\n",
    "\t\targ4 : int\n",
    "\t\t\tThe k value\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tfloat\n",
    "\t\t\tThe MAP value as a number between 0 and 1\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmeanAveragePrecision = -1\n",
    "\t\ttotal_AveragePrecision = 0\n",
    "\t\tcount = 0\n",
    "\t\tfor i in query_ids:\n",
    "\t\t\ttrue_doc_IDs = []\n",
    "\t\t\tflag = 0\n",
    "\t\t\tfor dic in qrels:\n",
    "\t\t\t\tif(int(dic[\"query_num\"]) == i):\n",
    "\t\t\t\t\tflag = 1\n",
    "\t\t\t\t\ttrue_doc_IDs.append(int(dic[\"id\"]))\n",
    "\t\t\t\telif(flag == 1):\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\ttotal_AveragePrecision = total_AveragePrecision + self.queryAveragePrecision(doc_IDs_ordered[count] , i , true_doc_IDs, k)\n",
    "\t\t\tcount = count+1\n",
    "\t\tif count == 0:\n",
    "\t  \t\tcount = 1\n",
    "\t\tmeanAveragePrecision = total_AveragePrecision/count\n",
    "\n",
    "\t\treturn meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 300 # Parameter to tune\n",
    "doc_vectors, query_vectors = LSI(n_components, T, S, Dt, tfidf_q)\n",
    "doc_IDs_ordered = rank(doc_vectors, query_vectors, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision, Recall and F-score @ 1 : 0.008888888888888889, 0.0011851851851851852, 0.002037019483176399\n",
      "MAP, nDCG @ 1 : 0.008888888888888889, 0.0077777777777777776\n",
      "Precision, Recall and F-score @ 2 : 0.006666666666666667, 0.0014814814814814816, 0.0023155655363588925\n",
      "MAP, nDCG @ 2 : 0.008888888888888889, 0.005847423866488529\n",
      "Precision, Recall and F-score @ 3 : 0.005925925925925926, 0.0019753086419753087, 0.0028394563280804224\n",
      "MAP, nDCG @ 3 : 0.010370370370370368, 0.005256966580840124\n",
      "Precision, Recall and F-score @ 4 : 0.0044444444444444444, 0.0019753086419753087, 0.0026070318640519555\n",
      "MAP, nDCG @ 4 : 0.010370370370370368, 0.004521649822037544\n",
      "Precision, Recall and F-score @ 5 : 0.0044444444444444444, 0.0023793490460157127, 0.002968175576160028\n",
      "MAP, nDCG @ 5 : 0.011259259259259259, 0.004612647814091636\n",
      "Precision, Recall and F-score @ 6 : 0.0074074074074074086, 0.00467678494345161, 0.00542926910591536\n",
      "MAP, nDCG @ 6 : 0.014962962962962961, 0.00606655254353258\n",
      "Precision, Recall and F-score @ 7 : 0.008253968253968253, 0.0056383234049900725, 0.006367128777113744\n",
      "MAP, nDCG @ 7 : 0.016497354497354497, 0.007034529849982212\n",
      "Precision, Recall and F-score @ 8 : 0.008333333333333333, 0.0061123974790641454, 0.00666891321461181\n",
      "MAP, nDCG @ 8 : 0.016867724867724865, 0.0070573629918148995\n",
      "Precision, Recall and F-score @ 9 : 0.009876543209876546, 0.010863720230386897, 0.009500615194220892\n",
      "MAP, nDCG @ 9 : 0.019336860670194, 0.008335126030663383\n",
      "Precision, Recall and F-score @ 10 : 0.010222222222222225, 0.013498640865307531, 0.010733366328017203\n",
      "MAP, nDCG @ 10 : 0.01992945326278659, 0.009208189678819333\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, f-score, MAP and nDCG for k = 1 to 10\n",
    "precisions, recalls, fscores, MAPs, nDCGs = [], [], [], [], []\n",
    "for k in range(1, 11):\n",
    "    precision = evaluator.meanPrecision(doc_IDs_ordered, query_ids, qrels, k)\n",
    "    precisions.append(precision)\n",
    "    recall = evaluator.meanRecall(doc_IDs_ordered, query_ids, qrels, k)\n",
    "    recalls.append(recall)\n",
    "    fscore = evaluator.meanFscore(doc_IDs_ordered, query_ids, qrels, k)\n",
    "    fscores.append(fscore)\n",
    "    print(\"Precision, Recall and F-score @ \" +  str(k) + \" : \" + str(precision) + \", \" + str(recall) + \", \" + str(fscore))\n",
    "    MAP = evaluator.meanAveragePrecision(doc_IDs_ordered, query_ids, qrels, k)\n",
    "    MAPs.append(MAP)\n",
    "    nDCG = evaluator.meanNDCG(doc_IDs_ordered, query_ids, qrels, k)\n",
    "    nDCGs.append(nDCG)\n",
    "    print(\"MAP, nDCG @ \" +  str(k) + \" : \" + str(MAP) + \", \" + str(nDCG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = np.reshape(np.array([4.0,5.0,1.0,7.0]), (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
